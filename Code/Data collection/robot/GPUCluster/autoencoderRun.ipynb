{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n",
      "GPU: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "path=\"/its/home/drs25/Documents/GitHub/TactileSensor/Code/Data collection/robot/\"\n",
    "if sys.platform.startswith('win'):\n",
    "    path=\"C:/Users/dexte/Documents/GitHub/TactileSensor/Code/Data collection/robot/\"\n",
    "    \n",
    "torch.cuda.empty_cache() \n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=torch.device(\"cpu\")\n",
    "print(torch.version.cuda)\n",
    "print(\"GPU:\",torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use\n",
    "Most things are set up lready to work. You will need a pretrained autoencoder network. This will then download the same dataset and create a model that uses the pretrained encoder for a meaningful neural network. \n",
    "\n",
    "#### ToDo\n",
    "Needs to finish modified encoder\n",
    "Needs to edit and test training loop\n",
    "Need to test on untrained unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(name,vibration=True,dir=\"all\"):\n",
    "    df = pd.read_csv(path+name)\n",
    "    df=pd.DataFrame(df).fillna(0)\n",
    "    if vibration:\n",
    "        x=np.array([df['s1'],df['s2'],df['s3'],df['s4'],df['s5'],df['s6']])\n",
    "    else:\n",
    "        x=np.array([df['s1'],df['s2'],df['s5'],df['s6']])\n",
    "    if dir==\"left\":\n",
    "        x=np.array([df['s1'],df['s2'],df['s3']])\n",
    "    elif dir==\"right\":\n",
    "        x=np.array([df['s4'],df['s5'],df['s6']])\n",
    "    x=x.T #transpose to have layers\n",
    "    y=np.array([df['x'],df['y'],df['z']])\n",
    "    y=y.T\n",
    "    nan_indices = np.where(np.isnan(y))\n",
    "    y[nan_indices]=0\n",
    "    print(\"X data:\",x.shape,\"/ny data:\",y.shape)\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def gen_temporal_data_2(X_,y_,T):\n",
    "    X=X_.copy()\n",
    "    temp_x=np.zeros((X.shape[0]-T,T,X.shape[1]))\n",
    "    temp_y=np.zeros((X.shape[0]-T,y_.shape[1]))\n",
    "    for j in range(len(y_)-T): #loop through classes\n",
    "        ar=[X[j+k] for k in range(T)]\n",
    "        temp_x[j]=np.array(ar)\n",
    "        temp_y[j]=y_[j]\n",
    "    x=temp_x\n",
    "    y=temp_y\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def augmented_pattern(x,y):\n",
    "    isMod=False\n",
    "    while not isMod:\n",
    "        chunk_size=np.random.randint(5,100)\n",
    "        if len(x[0])%chunk_size==0:\n",
    "            isMod=True\n",
    "    repetitions=np.random.randint(3,5)\n",
    "    print(chunk_size)\n",
    "    randomized_x = []\n",
    "    randomized_y = []\n",
    "    \n",
    "    for idx, pattern in enumerate(x):\n",
    "        pattern_length = len(pattern)\n",
    "        num_chunks = pattern_length // chunk_size\n",
    "        \n",
    "        # Split pattern into chunks of size chunk_size\n",
    "        chunks = [pattern[i * chunk_size: (i + 1) * chunk_size] for i in range(num_chunks)]\n",
    "        \n",
    "        for _ in range(repetitions):\n",
    "            # Shuffle the chunks randomly\n",
    "            np.random.shuffle(chunks)\n",
    "            # Concatenate the shuffled chunks to create a new pattern\n",
    "            randomized_pattern = np.concatenate(chunks)\n",
    "            \n",
    "            # Store the randomized pattern and corresponding label\n",
    "            randomized_x.append(randomized_pattern)\n",
    "            randomized_y.append(y[idx])\n",
    "    \n",
    "    return np.array(randomized_x), np.array(randomized_y)\n",
    "def augmented_noise(X_):\n",
    "    X=X_.copy()\n",
    "    return X+np.random.normal(np.average(X),np.std(X)+1,X.shape)\n",
    "\n",
    "def getAugmentedData(X,y,T):\n",
    "    x,y=gen_temporal_data_2(X,y,T)\n",
    "    x1=augmented_noise(x)\n",
    "    x2,y1=augmented_pattern(x,y)\n",
    "    x=np.concatenate([x,x1,x2])\n",
    "    y=np.concatenate([y,y,y1])\n",
    "    #reduction\n",
    "    X_=(x-np.average(x))/np.std(x)\n",
    "    y_=(y-np.average(y))/np.std(y)\n",
    "    #split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_.astype(np.float32), y_.astype(np.float32), test_size=0.2, random_state=42)\n",
    "    X_train=torch.tensor(X_train).to(device)\n",
    "    X_test=torch.tensor(X_test).to(device)\n",
    "    Y_train=torch.tensor(Y_train).to(device)\n",
    "    Y_test=torch.tensor(Y_test).to(device)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def getData(X,y,T):\n",
    "    X_,y_=gen_temporal_data_2(X,y,T)\n",
    "    #reduction\n",
    "    X_=(X_-np.average(X_))/np.std(X_)\n",
    "    y=(y-np.average(y))/np.std(y)\n",
    "    #split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_.astype(np.float32), y_.astype(np.float32), test_size=0.2, random_state=42)\n",
    "    X_train=torch.tensor(X_train)\n",
    "    X_test=torch.tensor(X_test)\n",
    "    Y_train=torch.tensor(Y_train)\n",
    "    Y_test=torch.tensor(Y_test)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "# Define your Autoencoder class\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_size,output_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, latent_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, input_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_size, input_size)  # Output size is 3\n",
    "        )\n",
    "        #self.endlayer = nn.linear(input_size,output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "input_size = 784  # Adjust this based on your input data size\n",
    "encoding_dim = 32  # Adjust the encoding dimension as needed\n",
    "autoencoder = Autoencoder(input_size, encoding_dim)\n",
    "\n",
    "#setup new class \n",
    "class ModifiedEncoder(nn.Module):\n",
    "    def __init__(self, base_encoder, num_classes):\n",
    "        super(ModifiedEncoder, self).__init__()\n",
    "        self.base_encoder = base_encoder\n",
    "        self.classification_layer = nn.Linear(encoding_dim, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.base_encoder(x)\n",
    "        classification_output = self.classification_layer(encoded)\n",
    "        return self.softmax(classification_output)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X,y=sort_data(\"accmovementLeftFoot.csv\")\n",
    "X1,y1=sort_data(\"accmovementRightFoot.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"accmovementRightFootCarpet.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"accmovementLeftFootCarpet.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"accmovementRightFootConrete.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"accmovementLeftFootConrete.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"accmovementRightFootOutdoor.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"accmovementLeftFootOutdoor.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"accmovementLeftFootCarpetDay2.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"accmovementRightFootCarpetDay2.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"accmovementLeftFootCarpetDay3.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"accmovementRightFootCarpetDay3.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"accmovementLeftFootDay4.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"accmovementRightFootDay4.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 50 * 6\n",
    "latent_size = 64  # Choose an appropriate size for the latent space\n",
    "#load trained auto encoder\n",
    "autoencoder = Autoencoder(input_size, latent_size).to(device)\n",
    "autoencoder.load_state_dict(torch.load(path+\"GPUCluster/data/\"+\"autoencoder_model.pth\",map_location=torch.device('cpu')))\n",
    "#create normal model\n",
    "output=3\n",
    "model=ModifiedEncoder(autoencoder,output)\n",
    "model.base_encoder.load_state_dict(autoencoder.encoder.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test=getAugmentedData(X,y,50)\n",
    "\n",
    "# Define your optimizer and loss function\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "history=[]\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        inputs = X_train[i:i + batch_size]\n",
    "        targets = Y_train[i:i + batch_size]\n",
    "\n",
    "        output = autoencoder(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        #loss_x = criterion(output, x)\n",
    "        loss = criterion(output, targets)  # Use only the first 3 elements for Y\n",
    "        #loss = loss_x + loss_y  # Combine both losses\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        history.append(loss.item())\n",
    "        # Print progress\n",
    "    if epoch%100==0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "torch.save(autoencoder.state_dict(), path+\"GPUCluster/data/\"+\"autoencoder_mod_model.pth\")\n",
    "\n",
    "np.save(path+\"GPUCluster/data/train_loss_new\",np.array(history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test=getAugmentedData(X,y,50)\n",
    "outputs = autoencoder.forward(X_train)\n",
    "\n",
    "plt.plot(outputs[:,0].cpu().detach().numpy(),\"--\",c=\"b\",label=\"Prediction x\")\n",
    "plt.plot(Y_train[:,0].cpu().detach().numpy(),c=\"b\",label=\"Actual x\")\n",
    "\n",
    "plt.plot(outputs[:,1].cpu().detach().numpy(),\"--\",c=\"r\",label=\"Prediction y\")\n",
    "plt.plot(Y_train[:,1].cpu().detach().numpy(),c=\"r\",label=\"Actual y\")\n",
    "\n",
    "plt.plot(outputs[:,2].cpu().detach().numpy(),\"--\",c=\"g\",label=\"Prediction z\")\n",
    "plt.plot(Y_train[:,2].cpu().detach().numpy(),c=\"g\",label=\"Actual z\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Analogue value normalised\")\n",
    "plt.title(\"LSTM prediction vs real gyroscope data\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
