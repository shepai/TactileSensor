{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier code for training the datasets\n",
    "\n",
    "## Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['time_step', 'x', 'y', 'z', 's_1_0', 's_1_1', 's_1_2', 's_1_3', 's_1_4',\n",
      "       's_1_5', 's_1_6', 's_1_7', 's_1_8', 's_1_9', 's_1_10', 's_1_11',\n",
      "       's_1_12', 's_1_13', 's_1_14', 's_1_15', 's_2_0', 's_2_1', 's_2_2',\n",
      "       's_2_3', 's_2_4', 's_2_5', 's_2_6', 's_2_7', 's_2_8', 's_2_9', 's_2_10',\n",
      "       's_2_11', 's_2_12', 's_2_13', 's_2_14', 's_2_15'],\n",
      "      dtype='object')\n",
      "11.7\n",
      "GPU: True\n",
      "X data: (4950, 32) /ny data: (4950, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (4950, 32) /ny data: (4950, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (4950, 32) /ny data: (4950, 3)\n",
      "(25590, 1920) (25590, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import sys\n",
    "#df = pd.read_csv('/its/home/drs25/Documents/GitHub/TactileSensor/Code/Data collection/robot/movementLeftFoot.csv')\n",
    "#df=(df-df.std())/(df.mean())\n",
    "path=\"/its/home/drs25/Documents/GitHub/TactileSensor/Code/Data collection/robot/New_feet/raw/\"\n",
    "if sys.platform.startswith('win'):\n",
    "    path=\"C:/Users/dexte/Documents/GitHub/TactileSensor/Code/Data collection/robot/New_feet/raw/\"\n",
    "df = pd.read_csv(path+'carpet_d1.1_raw_L.csv')\n",
    "#df=(df-df.std())/(df.mean())\n",
    "\n",
    "print(df.keys())\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.version.cuda)\n",
    "print(\"GPU:\",torch.cuda.is_available())\n",
    "\n",
    "\n",
    "def genDataSet(var):\n",
    "    X,y=sort_data(\"carpet_d1.1_raw_L.csv\",var)\n",
    "    X1,y1=sort_data(\"carpet_d1.1_raw_R.csv\",var)\n",
    "    X=np.concatenate((X,X1),axis=0)\n",
    "    y=np.concatenate((y,y1),axis=0)\n",
    "    X1,y1=sort_data(\"carpet_d1_raw_L.csv\",var)\n",
    "    X=np.concatenate((X,X1),axis=0)\n",
    "    y=np.concatenate((y,y1),axis=0)\n",
    "    X1,y1=sort_data(\"carpet_d1_raw_R.csv\",var)\n",
    "    X=np.concatenate((X,X1),axis=0)\n",
    "    y=np.concatenate((y,y1),axis=0)\n",
    "    X1,y1=sort_data(\"wood_d1_raw_R.csv\",var)\n",
    "    X=np.concatenate((X,X1),axis=0)\n",
    "    y=np.concatenate((y,y1),axis=0)\n",
    "    X1,y1=sort_data(\"wood_d1_raw_L.csv\",var)\n",
    "    X=np.concatenate((X,X1),axis=0)\n",
    "    y=np.concatenate((y,y1),axis=0)\n",
    "    return X,y\n",
    "def sort_data(name,vibration=True,dir=\"all\"):\n",
    "    df = pd.read_csv(path+name)\n",
    "    df=pd.DataFrame(df).fillna(0)\n",
    "    if vibration:\n",
    "        ar=[]\n",
    "        for key in list(df.keys()):\n",
    "            if key not in [\"time_step\",\"x\",\"y\",\"z\"]:\n",
    "                ar.append(df[key])\n",
    "        x=np.array(ar)\n",
    "    else: #return without vibration data\n",
    "        ar=[]\n",
    "        for key in list(df.keys()):\n",
    "            if key not in [\"time_step\",\"x\",\"y\",\"z\",\"s_1_7\",\"s_2_7\"]:\n",
    "                ar.append(df[key])\n",
    "        x=np.array(ar)\n",
    "    if dir==\"left\":\n",
    "        ar=[]\n",
    "        for key in list(df.keys()):\n",
    "            if key not in [\"time_step\",\"x\",\"y\",\"z\"] and \"s_1\" in key:\n",
    "                ar.append(df[key])\n",
    "        x=np.array(ar)\n",
    "    elif dir==\"right\":\n",
    "        ar=[]\n",
    "        for key in list(df.keys()):\n",
    "            if key not in [\"x\",\"y\",\"z\"] and \"s_2\" in key:\n",
    "                ar.append(df[key])\n",
    "        x=np.array(ar)\n",
    "    x=x.T #transpose to have layers\n",
    "    y=np.array([df['x'],df['y'],df['z']])\n",
    "    y=y.T\n",
    "    nan_indices = np.where(np.isnan(y))\n",
    "    y[nan_indices]=0\n",
    "    print(\"X data:\",x.shape,\"/ny data:\",y.shape)\n",
    "    return x,y\n",
    "\n",
    "X,y=sort_data(\"carpet_d1.1_raw_L.csv\")\n",
    "X1,y1=sort_data(\"carpet_d1.1_raw_R.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"carpet_d1_raw_L.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"carpet_d1_raw_R.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"wood_d1_raw_R.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"wood_d1_raw_L.csv\")\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "\n",
    "def gen_temporal_data(X_,y_,T):\n",
    "    X=X_.copy()\n",
    "    temp_x=np.zeros((X.shape[0]-T,X.shape[1]*T))\n",
    "    temp_y=np.zeros((X.shape[0]-T,y_.shape[1]))\n",
    "    for j in range(len(y_)-T): #loop through classes\n",
    "        ar=[X[j+k] for k in range(T)]\n",
    "        temp_x[j]=np.concatenate(ar,axis=0)\n",
    "        temp_y[j]=y_[j]\n",
    "    x=temp_x\n",
    "    y=temp_y\n",
    "    return x, y\n",
    "\n",
    "def gen_temporal_data_2(X_,y_,T):\n",
    "    X=X_.copy()\n",
    "    temp_x=np.zeros((X.shape[0]-T,T,X.shape[1]))\n",
    "    temp_y=np.zeros((X.shape[0]-T,y_.shape[1]))\n",
    "    for j in range(len(y_)-T): #loop through classes\n",
    "        ar=[X[j+k] for k in range(T)]\n",
    "        temp_x[j]=np.array(ar)\n",
    "        temp_y[j]=y_[j]\n",
    "    x=temp_x\n",
    "    y=temp_y\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def reduction_matrix(x_standard):\n",
    "    return np.average(x_standard,axis=0)\n",
    "\n",
    "X_,y_=gen_temporal_data(X,y,60)\n",
    "print(X_.shape,y_.shape)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "\n",
    "        # Create a list to store the layers of the neural network\n",
    "        layers = []\n",
    "\n",
    "        # Add the input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.Sigmoid())  # You can use other activation functions as well\n",
    "        #print(layers[0].weight.dtype)\n",
    "        # Add the hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            layers.append(nn.Sigmoid())  # You can use other activation functions as well\n",
    "\n",
    "        # Add the output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "\n",
    "        # Combine all layers into a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass of the network\n",
    "        return self.model(x)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagation\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify texture\n",
    "\n",
    "This section takes in a temporal window of tactile data and classifies which texture it is belonging to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (4950, 32) /ny data: (4950, 3)\n",
      "X data: (4950, 32) /ny data: (4950, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (4950, 32) /ny data: (4950, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (4950, 32) /ny data: (4950, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (4950, 32) /ny data: (4950, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (4950, 32) /ny data: (4950, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n"
     ]
    }
   ],
   "source": [
    "var=1\n",
    "X,y=sort_data(\"wood_d1_raw_R.csv\",var)\n",
    "X1,y1=sort_data(\"wood_d1_raw_L.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"wood_d2.1_raw_L.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"wood_d2.1_raw_R.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"wood_d2.3_raw_R.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"wood_d2_raw_R.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "\n",
    "wood_X=X\n",
    "wood_y=y.copy()\n",
    "\n",
    "X1,yy=sort_data(\"carpet_d1.1_raw_L.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"carpet_d1.1_raw_R.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"carpet_d1_raw_L.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"carpet_d1_raw_R.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "\n",
    "carpet_x=X.copy()\n",
    "carpet_y=y.copy()\n",
    "\n",
    "X1,y1=sort_data(\"concrete_d2_raw_L.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"concrete_d2_raw_R.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"concrete_d2.1_raw_L.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"concrete_d2.1_raw_R.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "\n",
    "concrete_X=X.copy()\n",
    "concrete_y=y.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forrest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 16.79%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X=np.concatenate([concrete_X,carpet_x,wood_X])\n",
    "y1=np.zeros((len(concrete_X),3))\n",
    "y1[:,0]=1\n",
    "y2=np.zeros((len(carpet_x),3))\n",
    "y2[:,1]=1\n",
    "y3=np.zeros((len(wood_X),3))\n",
    "y3[:,2]=1\n",
    "y=np.concatenate([y1,y2,y3])\n",
    "\n",
    "X,y=gen_temporal_data(X,y,80)\n",
    "\n",
    "X=(X-np.mean(X))/(np.std(X))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dexte\\AppData\\Local\\Temp\\ipykernel_23524\\1130973042.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train=torch.tensor(X_train,dtype=torch.float32).to(device)\n",
      "C:\\Users\\dexte\\AppData\\Local\\Temp\\ipykernel_23524\\1130973042.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train=torch.tensor(y_train,dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Loss: 1.2111\n",
      "Epoch [101/10000], Loss: 1.0392\n",
      "Epoch [201/10000], Loss: 1.0374\n",
      "Epoch [301/10000], Loss: 1.0357\n",
      "Epoch [401/10000], Loss: 1.0339\n",
      "Epoch [501/10000], Loss: 1.0319\n",
      "Epoch [601/10000], Loss: 1.0298\n",
      "Epoch [701/10000], Loss: 1.0275\n",
      "Epoch [801/10000], Loss: 1.0250\n",
      "Epoch [901/10000], Loss: 1.0223\n",
      "Epoch [1001/10000], Loss: 1.0193\n",
      "Epoch [1101/10000], Loss: 1.0161\n",
      "Epoch [1201/10000], Loss: 1.0126\n",
      "Epoch [1301/10000], Loss: 1.0090\n",
      "Epoch [1401/10000], Loss: 1.0051\n",
      "Epoch [1501/10000], Loss: 1.0012\n",
      "Epoch [1601/10000], Loss: 0.9972\n",
      "Epoch [1701/10000], Loss: 0.9932\n",
      "Epoch [1801/10000], Loss: 0.9891\n",
      "Epoch [1901/10000], Loss: 0.9852\n",
      "Epoch [2001/10000], Loss: 0.9813\n",
      "Epoch [2101/10000], Loss: 0.9776\n",
      "Epoch [2201/10000], Loss: 0.9740\n",
      "Epoch [2301/10000], Loss: 0.9705\n",
      "Epoch [2401/10000], Loss: 0.9672\n",
      "Epoch [2501/10000], Loss: 0.9641\n",
      "Epoch [2601/10000], Loss: 0.9612\n",
      "Epoch [2701/10000], Loss: 0.9585\n",
      "Epoch [2801/10000], Loss: 0.9560\n",
      "Epoch [2901/10000], Loss: 0.9537\n",
      "Epoch [3001/10000], Loss: 0.9516\n",
      "Epoch [3101/10000], Loss: 0.9497\n",
      "Epoch [3201/10000], Loss: 0.9480\n",
      "Epoch [3301/10000], Loss: 0.9463\n",
      "Epoch [3401/10000], Loss: 0.9447\n",
      "Epoch [3501/10000], Loss: 0.9433\n",
      "Epoch [3601/10000], Loss: 0.9418\n",
      "Epoch [3701/10000], Loss: 0.9405\n",
      "Epoch [3801/10000], Loss: 0.9391\n",
      "Epoch [3901/10000], Loss: 0.9378\n",
      "Epoch [4001/10000], Loss: 0.9366\n",
      "Epoch [4101/10000], Loss: 0.9353\n",
      "Epoch [4201/10000], Loss: 0.9340\n",
      "Epoch [4301/10000], Loss: 0.9327\n",
      "Epoch [4401/10000], Loss: 0.9315\n",
      "Epoch [4501/10000], Loss: 0.9302\n",
      "Epoch [4601/10000], Loss: 0.9289\n",
      "Epoch [4701/10000], Loss: 0.9276\n",
      "Epoch [4801/10000], Loss: 0.9263\n",
      "Epoch [4901/10000], Loss: 0.9249\n",
      "Epoch [5001/10000], Loss: 0.9236\n",
      "Epoch [5101/10000], Loss: 0.9222\n",
      "Epoch [5201/10000], Loss: 0.9208\n",
      "Epoch [5301/10000], Loss: 0.9194\n",
      "Epoch [5401/10000], Loss: 0.9179\n",
      "Epoch [5501/10000], Loss: 0.9165\n",
      "Epoch [5601/10000], Loss: 0.9150\n",
      "Epoch [5701/10000], Loss: 0.9135\n",
      "Epoch [5801/10000], Loss: 0.9120\n",
      "Epoch [5901/10000], Loss: 0.9104\n",
      "Epoch [6001/10000], Loss: 0.9089\n",
      "Epoch [6101/10000], Loss: 0.9074\n",
      "Epoch [6201/10000], Loss: 0.9059\n",
      "Epoch [6301/10000], Loss: 0.9043\n",
      "Epoch [6401/10000], Loss: 0.9028\n",
      "Epoch [6501/10000], Loss: 0.9013\n",
      "Epoch [6601/10000], Loss: 0.8998\n",
      "Epoch [6701/10000], Loss: 0.8984\n",
      "Epoch [6801/10000], Loss: 0.8970\n",
      "Epoch [6901/10000], Loss: 0.8956\n",
      "Epoch [7001/10000], Loss: 0.8942\n",
      "Epoch [7101/10000], Loss: 0.8929\n",
      "Epoch [7201/10000], Loss: 0.8916\n",
      "Epoch [7301/10000], Loss: 0.8903\n",
      "Epoch [7401/10000], Loss: 0.8891\n",
      "Epoch [7501/10000], Loss: 0.8880\n",
      "Epoch [7601/10000], Loss: 0.8868\n",
      "Epoch [7701/10000], Loss: 0.8858\n",
      "Epoch [7801/10000], Loss: 0.8847\n",
      "Epoch [7901/10000], Loss: 0.8837\n",
      "Epoch [8001/10000], Loss: 0.8827\n",
      "Epoch [8101/10000], Loss: 0.8818\n",
      "Epoch [8201/10000], Loss: 0.8809\n",
      "Epoch [8301/10000], Loss: 0.8801\n",
      "Epoch [8401/10000], Loss: 0.8793\n",
      "Epoch [8501/10000], Loss: 0.8785\n",
      "Epoch [8601/10000], Loss: 0.8777\n",
      "Epoch [8701/10000], Loss: 0.8770\n",
      "Epoch [8801/10000], Loss: 0.8763\n",
      "Epoch [8901/10000], Loss: 0.8756\n",
      "Epoch [9001/10000], Loss: 0.8750\n",
      "Epoch [9101/10000], Loss: 0.8744\n",
      "Epoch [9201/10000], Loss: 0.8738\n",
      "Epoch [9301/10000], Loss: 0.8732\n",
      "Epoch [9401/10000], Loss: 0.8726\n",
      "Epoch [9501/10000], Loss: 0.8721\n",
      "Epoch [9601/10000], Loss: 0.8716\n",
      "Epoch [9701/10000], Loss: 0.8711\n",
      "Epoch [9801/10000], Loss: 0.8706\n",
      "Epoch [9901/10000], Loss: 0.8701\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train=torch.tensor(X_train,dtype=torch.float32).to(device)\n",
    "y_train=torch.tensor(y_train,dtype=torch.float32).to(device)\n",
    "\n",
    "model = SimpleNN(len(X_train[0]), [1000,50], 3).to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs=10000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    average_loss = total_loss \n",
    "    if epoch%100==0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dexte\\AppData\\Local\\Temp\\ipykernel_23524\\3738214504.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test=torch.tensor(X_test,dtype=torch.float32).to(device)\n",
      "C:\\Users\\dexte\\AppData\\Local\\Temp\\ipykernel_23524\\3738214504.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test=torch.tensor(y_test,dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 47.107637769900094 %\n"
     ]
    }
   ],
   "source": [
    "X_test=torch.tensor(X_test,dtype=torch.float32).to(device)\n",
    "y_test=torch.tensor(y_test,dtype=torch.float32).to(device)\n",
    "\n",
    "predictions=model(X_test)\n",
    "ind=torch.argmax(predictions,axis=1).cpu().detach().numpy()\n",
    "pred=np.zeros_like(predictions.cpu().detach().numpy())\n",
    "for i in range(len(pred)):\n",
    "    pred[i][ind[i]]=1\n",
    "\n",
    "accuracy = accuracy_score(y_test.cpu().detach().numpy(), pred)\n",
    "print(\"Acc:\",accuracy*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train,Y_train,X_test,Y_test,num_epochs = 100,learning_rate = 0.01):\n",
    "    # Split your dataset into training and validation sets\n",
    "    # train_data, val_data = ...\n",
    "    lstm_model=LSTMModel(X_train.shape[2],64,2,Y_train.shape[1]).to(device)\n",
    "    # Create data loaders for training and validation\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = optim.SGD(lstm_model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    history_train=[]\n",
    "    history_test=[]\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        lstm_model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = lstm_model(X_train)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, Y_train)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss = loss.item()\n",
    "        history_train.append(total_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        lstm_model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "\n",
    "            outputs = lstm_model(X_test)\n",
    "            val_loss = criterion(outputs, Y_test)\n",
    "            total_val_loss = val_loss.item()\n",
    "\n",
    "            print(f\"Validation Loss: {total_val_loss:.4f}\")\n",
    "            history_test.append(total_val_loss)\n",
    "    # Save the trained model\n",
    "    #torch.save(lstm_model.state_dict(), \"lstm_model.pth\")\n",
    "    return history_train, history_test\n",
    "\n",
    "lossTrain,lossTest=train(X_train,Y_train,X_test,Y_test,num_epochs = 900000,learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify edges\n",
    "\n",
    "This section we experiment with a regression model to classify simply where an edge is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 15) (88, 4)\n",
      "Accuracy test: 1.0\n",
      "Accuracy train: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filepath=\"/its/home/drs25/Documents/GitHub/TactileSensor/Code/Data collection/edges/\"\n",
    "if sys.platform.startswith('win'): filepath=\"C:/Users/dexte/Documents/GitHub/TactileSensor/Code/Data collection/edges/\"\n",
    "x=np.load(filepath+\"xdata1.npy\")\n",
    "y=np.load(filepath+\"ydata1.npy\")\n",
    "\n",
    "x_lin=x.reshape((len(x),3*5))\n",
    "x_noise=x_lin+np.random.normal(0,10,x_lin.shape)\n",
    "\n",
    "x=np.concatenate([x_lin,x_noise])\n",
    "y=np.concatenate([y,y])\n",
    "\n",
    "x_train=x[0:int(len(x)*0.8)]\n",
    "y_train=y[0:int(len(x)*0.8)]\n",
    "x_test=x[int(len(x)*0.8):]\n",
    "y_test=y[int(len(x)*0.8):]\n",
    "print(x_lin.shape,y.shape)\n",
    "alpha = 1.0  # Ridge regularization parameter (adjust as needed)\n",
    "ridge_model = Ridge(alpha=alpha)\n",
    "#ridge_model = LinearRegression(positive=True)\n",
    "# Fit the model to the training data\n",
    "ridge_model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "Y_pred = ridge_model.predict(x_test)\n",
    "\n",
    "Y_pred[Y_pred<0.5]=0\n",
    "Y_pred[Y_pred>=0.5]=1\n",
    "incorrect=np.sum(Y_pred-y_test)/len(Y_pred)\n",
    "print(\"Accuracy test:\",1-incorrect)\n",
    "\n",
    "Y_pred = ridge_model.predict(x_train)\n",
    "\n",
    "Y_pred[Y_pred<0.5]=0\n",
    "Y_pred[Y_pred>=0.5]=1\n",
    "incorrect=np.sum(Y_pred-y_train)/len(Y_pred)\n",
    "print(\"Accuracy train:\",1-incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=np.load(filepath+\"xdata1.npy\")\n",
    "y=np.load(filepath+\"ydata1.npy\")\n",
    "\n",
    "#mean_values = np.mean(x, axis=1, keepdims=True)\n",
    "#std_dev_values = np.std(x, axis=1, keepdims=True)\n",
    "#x = (x - mean_values) / std_dev_values\n",
    "\n",
    "n=np.zeros_like(x[0])\n",
    "s=np.zeros_like(x[0])\n",
    "w=np.zeros_like(x[0])\n",
    "e=np.zeros_like(x[0])\n",
    "for i in range(len(x)):\n",
    "    if list(y[i])==[1,1,0,0]: \n",
    "        sig=x[i]\n",
    "        #sig[sig!=np.max(sig)]=0\n",
    "        n+=sig\n",
    "    elif list(y[i])==[0,1,1,0]: \n",
    "        sig=x[i]\n",
    "        #sig[sig!=np.max(sig)]=0\n",
    "        e+=sig\n",
    "    elif list(y[i])==[0,0,1,1]: \n",
    "        sig=x[i]\n",
    "        #sig[sig!=np.max(sig)]=0\n",
    "        s+=sig\n",
    "    elif list(y[i])==[1,0,0,1]: \n",
    "        sig=x[i]\n",
    "        #sig[sig!=np.max(sig)]=0\n",
    "        w+=sig\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 6))\n",
    "\n",
    "axs[0][0].imshow(n,cmap='inferno')\n",
    "axs[0][0].set_title(\"North\")\n",
    "axs[0][0].xaxis.set_visible(False)\n",
    "axs[0][0].yaxis.set_visible(False)\n",
    "\n",
    "axs[0][1].imshow(e,cmap='inferno')\n",
    "axs[0][1].set_title(\"East\")\n",
    "axs[0][1].xaxis.set_visible(False)\n",
    "axs[0][1].yaxis.set_visible(False)\n",
    "\n",
    "axs[1][0].imshow(s,cmap='inferno')\n",
    "axs[1][0].set_title(\"South\")\n",
    "axs[1][0].xaxis.set_visible(False)\n",
    "axs[1][0].yaxis.set_visible(False)\n",
    "\n",
    "axs[1][1].imshow(w,cmap='inferno')\n",
    "axs[1][1].set_title(\"West\")\n",
    "axs[1][1].xaxis.set_visible(False)\n",
    "axs[1][1].yaxis.set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify orientation\n",
    "\n",
    "Continuing the previous work on classifying what the robots orientation is based off data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
